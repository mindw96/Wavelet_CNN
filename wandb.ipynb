{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def WaveletTransformAxisY(batch_img):\n",
    "    odd_img  = batch_img[:,0::2]\n",
    "    even_img = batch_img[:,1::2]\n",
    "    L = (odd_img + even_img) / 2.0\n",
    "    H = torch.abs(odd_img - even_img)\n",
    "\n",
    "    return L, H"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def WaveletTransformAxisX(batch_img):\n",
    "    # transpose + fliplr\n",
    "    tmp_batch = torch.permute(batch_img, (0, 2, 1))\n",
    "    tmp_batch = torch.fliplr(tmp_batch)\n",
    "    _dst_L, _dst_H = WaveletTransformAxisY(tmp_batch)\n",
    "    # transpose + flipud\n",
    "    dst_L = torch.permute(_dst_L, [0, 2, 1])\n",
    "    dst_L = torch.flipud(dst_L)\n",
    "    dst_H = torch.permute(_dst_H, [0, 2, 1])\n",
    "    dst_H = torch.flipud(dst_H)\n",
    "\n",
    "    return dst_L, dst_H"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def Wavelet(batch_image):\n",
    "    r = batch_image[:,0]\n",
    "    g = batch_image[:,1]\n",
    "    b = batch_image[:,2]\n",
    "\n",
    "    # level 1 decomposition\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(r)\n",
    "    r_wavelet_LL, r_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    r_wavelet_HL, r_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(g)\n",
    "    g_wavelet_LL, g_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    g_wavelet_HL, g_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(b)\n",
    "    b_wavelet_LL, b_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    b_wavelet_HL, b_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_data = [r_wavelet_LL, r_wavelet_LH, r_wavelet_HL, r_wavelet_HH,\n",
    "                    g_wavelet_LL, g_wavelet_LH, g_wavelet_HL, g_wavelet_HH,\n",
    "                    b_wavelet_LL, b_wavelet_LH, b_wavelet_HL, b_wavelet_HH]\n",
    "    transform_batch = torch.stack(wavelet_data, axis=1)\n",
    "\n",
    "    # level 2 decomposition\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(r_wavelet_LL)\n",
    "    r_wavelet_LL2, r_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    r_wavelet_HL2, r_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(g_wavelet_LL)\n",
    "    g_wavelet_LL2, g_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    g_wavelet_HL2, g_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(b_wavelet_LL)\n",
    "    b_wavelet_LL2, b_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    b_wavelet_HL2, b_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "\n",
    "    wavelet_data_l2 = [r_wavelet_LL2, r_wavelet_LH2, r_wavelet_HL2, r_wavelet_HH2,\n",
    "                       g_wavelet_LL2, g_wavelet_LH2, g_wavelet_HL2, g_wavelet_HH2,\n",
    "                       b_wavelet_LL2, b_wavelet_LH2, b_wavelet_HL2, b_wavelet_HH2]\n",
    "    transform_batch_l2 = torch.stack(wavelet_data_l2, dim=1)\n",
    "\n",
    "    # level 3 decomposition\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(r_wavelet_LL2)\n",
    "    r_wavelet_LL3, r_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    r_wavelet_HL3, r_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(g_wavelet_LL2)\n",
    "    g_wavelet_LL3, g_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    g_wavelet_HL3, g_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(b_wavelet_LL2)\n",
    "    b_wavelet_LL3, b_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    b_wavelet_HL3, b_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_data_l3 = [r_wavelet_LL3, r_wavelet_LH3, r_wavelet_HL3, r_wavelet_HH3,\n",
    "                       g_wavelet_LL3, g_wavelet_LH3, g_wavelet_HL3, g_wavelet_HH3,\n",
    "                       b_wavelet_LL3, b_wavelet_LH3, b_wavelet_HL3, b_wavelet_HH3]\n",
    "    transform_batch_l3 = torch.stack(wavelet_data_l3, dim=1)\n",
    "\n",
    "    # level 4 decomposition\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(r_wavelet_LL3)\n",
    "    r_wavelet_LL4, r_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    r_wavelet_HL4, r_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(g_wavelet_LL3)\n",
    "    g_wavelet_LL4, g_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    g_wavelet_HL4, g_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(b_wavelet_LL3)\n",
    "    b_wavelet_LL4, b_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    b_wavelet_HL4, b_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "\n",
    "    wavelet_data_l4 = [r_wavelet_LL4, r_wavelet_LH4, r_wavelet_HL4, r_wavelet_HH4,\n",
    "                       g_wavelet_LL4, g_wavelet_LH4, g_wavelet_HL4, g_wavelet_HH4,\n",
    "                       b_wavelet_LL4, b_wavelet_LH4, b_wavelet_HL4, b_wavelet_HH4]\n",
    "    transform_batch_l4 = torch.stack(wavelet_data_l4, dim=1)\n",
    "\n",
    "    return [transform_batch, transform_batch_l2, transform_batch_l3, transform_batch_l4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Wavelet_Model(torch.nn.Module):\n",
    "    def __init__(self, classes=10):\n",
    "        super(Wavelet_Model, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(12, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_1 = nn.BatchNorm2d(64)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "        self.conv_1_2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.norm_1_2 = nn.BatchNorm2d(64)\n",
    "        self.relu_1_2 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_a = nn.Conv2d(12, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_a = nn.BatchNorm2d(64)\n",
    "        self.relu_a = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_2 = nn.BatchNorm2d(128)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        self.conv_2_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.norm_2_2 = nn.BatchNorm2d(128)\n",
    "        self.relu_2_2 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_b = nn.Conv2d(12, 128, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_b = nn.BatchNorm2d(128)\n",
    "        self.relu_b = nn.ReLU()\n",
    "\n",
    "        self.conv_b_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_b_2 = nn.BatchNorm2d(128)\n",
    "        self.relu_b_2 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_3 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_3 = nn.BatchNorm2d(256)\n",
    "        self.relu_3 = nn.ReLU()\n",
    "\n",
    "        self.conv_3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.norm_3_2 = nn.BatchNorm2d(256)\n",
    "        self.relu_3_2 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_c = nn.Conv2d(12, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_c = nn.BatchNorm2d(256)\n",
    "        self.relu_c = nn.ReLU()\n",
    "\n",
    "        self.conv_c_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_c_2 = nn.BatchNorm2d(256)\n",
    "        self.relu_c_2 = nn.ReLU()\n",
    "\n",
    "        self.conv_c_3 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_c_3 = nn.BatchNorm2d(256)\n",
    "        self.relu_c_3 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_4 = nn.Conv2d(512, 256, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_4 = nn.BatchNorm2d(256)\n",
    "        self.relu_4 = nn.ReLU()\n",
    "\n",
    "        self.conv_4_2 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.norm_4_2 = nn.BatchNorm2d(128)\n",
    "        self.relu_4_2 = nn.ReLU()\n",
    "        #################################################################################################\n",
    "        self.conv_5 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=1)\n",
    "        self.norm_5 = nn.BatchNorm2d(128)\n",
    "        self.relu_5 = nn.ReLU()\n",
    "\n",
    "        self.pool_5 = nn.AvgPool2d(kernel_size=(7,7), stride=1, padding=1)\n",
    "        self.flat_5 = nn.Flatten()\n",
    "\n",
    "        self.fc_5 = nn.Linear(1152, 2048)\n",
    "        self.norm_5_1 = nn.BatchNorm1d(2048)\n",
    "        self.relu_5_1 = nn.ReLU()\n",
    "        self.drop_5 = nn.Dropout(0.5)\n",
    "        #################################################################################################\n",
    "        self.fc_6 = nn.Linear(2048, classes)\n",
    "        self.norm_6 = nn.BatchNorm1d(classes)\n",
    "        self.relu_6 = nn.ReLU()\n",
    "        self.drop_6 = nn.Dropout(0.5)\n",
    "        #################################################################################################\n",
    "        self.output_fc = nn.Linear(classes, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_l1, input_l2, input_l3, input_l4 = Wavelet(x)\n",
    "        #################################################################################################\n",
    "        # print('input shape: ', input_l1.shape)\n",
    "        out_1 = self.conv_1(input_l1)\n",
    "        # print('conv_1 output shape: ', out_1.shape)\n",
    "        out_1 = self.norm_1(out_1)\n",
    "        out_1 = self.relu_1(out_1)\n",
    "\n",
    "        out_1 = self.conv_1_2(out_1)\n",
    "        # print('conv_1_2 output shape: ', out_1.shape)\n",
    "        out_1 = self.norm_1_2(out_1)\n",
    "        out_1 = self.relu_1_2(out_1)\n",
    "        #################################################################################################\n",
    "        out_2 = self.conv_a(input_l2)\n",
    "        # print('conv_a output shape: ', out_2.shape)\n",
    "        out_2 = self.norm_a(out_2)\n",
    "        out_2 = self.relu_a(out_2)\n",
    "\n",
    "        cat_2 = torch.cat((out_1, out_2), 1)\n",
    "        # print('concatenate result: ', cat_2.shape)\n",
    "        out_2 = self.conv_2(cat_2)\n",
    "        # print('conv_2 output shape: ', out_2.shape)\n",
    "        out_2 = self.norm_2(out_2)\n",
    "        out_2 = self.relu_2(out_2)\n",
    "\n",
    "        out_2 = self.conv_2_2(out_2)\n",
    "        # print('conv_2_2 output shape: ', out_2.shape)\n",
    "        out_2 = self.norm_2_2(out_2)\n",
    "        out_2 = self.relu_2_2(out_2)\n",
    "        #################################################################################################\n",
    "        out_3 = self.conv_b(input_l3)\n",
    "        # print('conv_b output shape: ', out_3.shape)\n",
    "        out_3 = self.norm_b(out_3)\n",
    "        out_3 = self.relu_b(out_3)\n",
    "\n",
    "        out_3 = self.conv_b_2(out_3)\n",
    "        # print('conv_b_2 output shape: ', out_3.shape)\n",
    "        out_3 = self.norm_b_2(out_3)\n",
    "        out_3 = self.relu_b_2(out_3)\n",
    "        #################################################################################################\n",
    "        cat_3 = torch.cat((out_2, out_3), 1)\n",
    "        # print('concatenate result: ', cat_3.shape)\n",
    "        out_3 = self.conv_3(cat_3)\n",
    "        # print('conv_3 output shape: ', out_3.shape)\n",
    "        out_3 = self.norm_3(out_3)\n",
    "        out_3 = self.relu_3(out_3)\n",
    "\n",
    "        out_3 = self.conv_3_2(out_3)\n",
    "        # print('conv_3_2 output shape: ', out_3.shape)\n",
    "        out_3 = self.norm_3_2(out_3)\n",
    "        out_3 = self.relu_3_2(out_3)\n",
    "        #################################################################################################\n",
    "        out_4 = self.conv_c(input_l4)\n",
    "        # print('conv_c output shape: ', out_4.shape)\n",
    "        out_4 = self.norm_c(out_4)\n",
    "        out_4 = self.relu_c(out_4)\n",
    "\n",
    "        out_4 = self.conv_c_2(out_4)\n",
    "        # print('conv_c_2 output shape: ', out_4.shape)\n",
    "        out_4 = self.norm_c_2(out_4)\n",
    "        out_4 = self.relu_c_2(out_4)\n",
    "\n",
    "        out_4 = self.conv_c_3(out_4)\n",
    "        # print('conv_c_3 output shape: ', out_4.shape)\n",
    "        out_4 = self.norm_c_3(out_4)\n",
    "        out_4 = self.relu_c_3(out_4)\n",
    "        #################################################################################################\n",
    "        cat_4 = torch.cat((out_3, out_4), 1)\n",
    "        # print('concatenate result: ', cat_4.shape)\n",
    "        out_4 = self.conv_4(cat_4)\n",
    "        # print('conv_4 output shape: ', out_4.shape)\n",
    "        out_4 = self.norm_4(out_4)\n",
    "        out_4 = self.relu_4(out_4)\n",
    "\n",
    "        out_4 = self.conv_4_2(out_4)\n",
    "        # print('conv_4_2 output shape: ', out_4.shape)\n",
    "        out_4 = self.norm_4_2(out_4)\n",
    "        out_4 = self.relu_4_2(out_4)\n",
    "        #################################################################################################\n",
    "        out_5 = self.conv_5(out_4)\n",
    "        # print('conv_5 output shape: ', out_5.shape)\n",
    "        out_5 = self.norm_5(out_5)\n",
    "        out_5 = self.relu_5(out_5)\n",
    "\n",
    "        out_5 = self.pool_5(out_5)\n",
    "        out_5 = self.flat_5(out_5)\n",
    "        #################################################################################################\n",
    "        out_5 = self.fc_5(out_5)\n",
    "        # print('fc_5 output shape: ', out_5.shape)\n",
    "        out_5 = self.norm_5_1(out_5)\n",
    "        out_5 = self.relu_5_1(out_5)\n",
    "        out_5 = self.drop_5(out_5)\n",
    "\n",
    "        out_6 = self.fc_6(out_5)\n",
    "        # print('fc6 output shape: ', out_6.shape)\n",
    "        out_6 = self.norm_6(out_6)\n",
    "        out_6 = self.relu_6(out_6)\n",
    "        out_6 = self.drop_6(out_6)\n",
    "        #################################################################################################\n",
    "        output = self.output_fc(out_6)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmindw96\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "batch_size  = 512\n",
    "random_seed = 888\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=300,\n",
    "    classes=58,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.0005,\n",
    "    dataset=\"KTH-TIPS2+DTD\",\n",
    "    architecture=\"Wavelet_CNN\",\n",
    "    data_path = './dataset/'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Texture Classification\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, test_loader, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, test_loader)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    datasets = get_data(config.data_path)\n",
    "    train_loader = make_loader(datasets['train'], batch_size=config.batch_size)\n",
    "    valid_loader = make_loader(datasets['valid'], batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = Wavelet_Model(config.classes).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    return model, train_loader, valid_loader, criterion, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_data(data_path=''):\n",
    "    texture_dataset = torchvision.datasets.ImageFolder(\n",
    "        data_path,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]))\n",
    "    #  equiv to slicing with [::slice]\n",
    "    train_idx, val_idx = train_test_split(list(range(len(texture_dataset))), test_size=0.2, random_state=random_seed)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(texture_dataset, train_idx)\n",
    "    datasets['valid'] = Subset(texture_dataset, val_idx)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=20)\n",
    "    return loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for _, (images, labels) in enumerate(loader):\n",
    "\n",
    "            loss, accuracy = train_batch(images, labels, model, optimizer, criterion)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss, example_ct, epoch, accuracy)\n",
    "\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    correct, total = 0, 0\n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch, accuracy):\n",
    "    loss = float(loss)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test images: {100 * correct / total}%\")\n",
    "\n",
    "        wandb.log({\"test_accuracy\": correct / total})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, images, \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_pipeline(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}